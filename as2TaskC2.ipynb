{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C2 (Client)- run this after running C1(Server)\n",
    "\n",
    "This scrpit will receive the streaming data and write it into MongoDB.\n",
    "\n",
    "Before running the following codes, please start MongoDB in the termianl first.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import ast\n",
    "#The function will send the input list/dictionary to mongoBD server\n",
    "\n",
    "def sendRecord(document):\n",
    "    doc = ast.literal_eval(document)       \n",
    "    \n",
    "    client = MongoClient()  \n",
    "    db = client.fit5148_new               \n",
    "    collection = db.taskc\n",
    "    \n",
    "    if type(doc) == dict:\n",
    "         collection.insert_one(doc)\n",
    "\n",
    "    client.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import ast\n",
    "\n",
    "#reveices the streaming documents from the given port and send this to mongoDB server\n",
    "def streaming(batch_interval,appName,timeout):\n",
    "    \"\"\"\n",
    "    batch_interval -- the time between each batch\n",
    "    appName -- the name of the straming application. \n",
    "    timeout -- After how long that cancel the execution\n",
    "\n",
    "    \"\"\"\n",
    "    # SparkContext plays an important role in the execution of Spark Applications\n",
    "    # SparkContext is reaponsible for interacting with programs and spark clusters, including creating RDD\n",
    "    \n",
    "    # We add this line to avoid an error : \"Cannot run multiple SparkContexts at once\".\n",
    "    # If there is an existing spark context, we will reuse it instead of creating a new context.\n",
    "\n",
    "    sc = SparkContext.getOrCreate()\n",
    "\n",
    "    # create a new spark context if there is no existing spark context\n",
    "    if sc is None:\n",
    "        # local[*]: run Spark locally with as many working processors as logical cores on your machine.\n",
    "        sc = SparkContext(master=\"local[*]\", appName=appName)\n",
    "\n",
    "    # create a streamingcontext based on sparkcontext\n",
    "    # a batch interval of \"batch_interval\" seconds \n",
    "    ssc = StreamingContext(sc,batch_interval) \n",
    "     \n",
    "    # Create a DStream connecting to hostname:port\n",
    "    lines = ssc.socketTextStream('localhost', 9999) \n",
    "    \n",
    "    #foreachRDD: push data in each RDD to an external system\n",
    "    lines.foreachRDD(lambda rdd:  rdd.foreach(sendRecord))\n",
    "\n",
    "    \n",
    "    # Start the computation\n",
    "    ssc.start() \n",
    "    \n",
    "    # Wait for the computation to terminate. \n",
    "    # We have added a `timeout` to deliberately cancel the execution after \"timeout\" seconds. \n",
    "    ssc.awaitTermination(timeout=timeout)\n",
    "    \n",
    "    #manually stop the streaming context\n",
    "    ssc.stop()\n",
    "    sc.stop()\n",
    "    print('Finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish\n"
     ]
    }
   ],
   "source": [
    "streaming(1,'Task3Streaming',1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please run following after \"Finish\" was shown in the previous streaming function\n",
    "  Use python application to show that streamed data have been added into the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Testing\n",
    "from pprint import pprint\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient()  \n",
    "db = client.fit5148_db \n",
    "result = db.taskc\n",
    "cursor = result.find().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
